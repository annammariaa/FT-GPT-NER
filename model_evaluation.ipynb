{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b2d0bb0-77a3-427b-ae8e-a174dd36e669",
   "metadata": {},
   "source": [
    "# Model evaluation based on predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8fe555-1549-4125-b8a5-1afae8fea056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "import evaluate\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d30dd8-bf43-440d-90f1-1ab4698efb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "seqeval = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c68c0c-b3b4-4a39-9862-6b68f4459026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicted_data(test_file, result_file):\n",
    "    '''\n",
    "    Function for matching result and test file based on annotations and combining them for evaluation.\n",
    "    '''\n",
    "    with open(test_file, 'r', encoding='utf-8') as f:\n",
    "        testdata = [json.loads(line) for line in f]\n",
    "\n",
    "    with open(result_file, 'r', encoding='utf-8') as f:\n",
    "        resdata = [json.loads(line) for line in f]\n",
    "    \n",
    "    combined_data = []\n",
    "    count = 0\n",
    "    j = 0\n",
    "    \n",
    "    for i, res in enumerate(resdata):\n",
    "        \n",
    "        full_example = {}\n",
    "        matched = False\n",
    "       \n",
    "        while not matched:\n",
    "            if j >= len(testdata):\n",
    "                break\n",
    "            \n",
    "            test_og = testdata[j][\"messages\"][2][\"content\"]\n",
    "            res_og = res[\"original\"]\n",
    "        \n",
    "            if test_og == res_og:\n",
    "                full_example[\"text\"] = testdata[j][\"messages\"][1][\"content\"]\n",
    "                full_example[\"original\"] = test_og\n",
    "                full_example[\"predicted\"] = res[\"predicted\"]\n",
    "                full_example[\"original_index\"] = testdata[j][\"messages\"][3][\"content\"]\n",
    "                \n",
    "                combined_data.append(full_example)\n",
    "                count += 1\n",
    "                matched = True\n",
    "            else:\n",
    "                j +=1\n",
    "        \n",
    "    return combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a934b7c-6300-4652-86c9-74ab26f2b3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_span(labels, text, model, count_pred):\n",
    "    '''\n",
    "    Function to get predicted label spans for evaluation.\n",
    "    '''\n",
    "    original_labels = {} # New predicted labels\n",
    "    \n",
    "    found_problem = False # Only complete results are returned\n",
    "\n",
    "    if isinstance(labels, str):\n",
    "        labels = json.loads(labels)\n",
    "\n",
    "    text = text.strip()\n",
    "    text = text.lower()\n",
    "\n",
    "    if isinstance(labels, dict):\n",
    "        for label, entities in labels.items():\n",
    "            try:\n",
    "                if label in data_labels:\n",
    "                    if entities:\n",
    "                        # Incase the entites are as a string, not list\n",
    "                        if isinstance(entities, str):\n",
    "                            entities = entities.split(\", \")\n",
    "\n",
    "                        # Entites without duplicates\n",
    "                        entities = list(set(entities))\n",
    "\n",
    "                        # All entity spans are found in text and saved to result list\n",
    "                        for entity in entities:\n",
    "                            entity = entity.strip()\n",
    "                            entity = entity.lower()\n",
    "                            pattern = re.compile(re.escape(entity))\n",
    "\n",
    "                            # Entites shorter than 2 characters are excluded, becaause they would produce too many false positives.\n",
    "                            if len(list(pattern.finditer(text))) == 0 or len(entity) < 2:\n",
    "                                if count_pred:\n",
    "                                    not_in_text[model][label].append(entity)\n",
    "                            else:\n",
    "                                if count_pred:\n",
    "                                    in_text[model][label] += 1\n",
    "\n",
    "                                for match in pattern.finditer(text):\n",
    "                                    start, end = match.span()\n",
    "                                    parts = list(TreebankWordTokenizer().span_tokenize(entity))\n",
    "\n",
    "                                    # Entities that consist of multible words are split\n",
    "                                    if len(parts) > 1:\n",
    "                                        for part in parts:\n",
    "                                            original_labels[(start + part[0], start + part[1])] = label\n",
    "                                    else:\n",
    "                                        original_labels[(start, end)] = label\n",
    "            except Exception as e:\n",
    "                found_problem = True\n",
    "                return None\n",
    "    else:\n",
    "        found_problem = True\n",
    "        return None\n",
    "\n",
    "    if found_problem:\n",
    "        return None\n",
    "    \n",
    "    return original_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf0b437-fc78-4bbc-b8f9-dbb0da470dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_predictions(test_file, result_file):\n",
    "    '''\n",
    "    Function that formats original and predicted values for evaluation with seqeval.\n",
    "    '''\n",
    "    \n",
    "    combined_data = predicted_data(test_file, result_file)\n",
    "    model = result_file.split(\"_\")[2]\n",
    "\n",
    "    original_by_token = [] # Original labels\n",
    "    predicted_by_token = [] # Predicted labels\n",
    "    examples_by_token = [] # Tokens\n",
    "    \n",
    "    for ie, example in enumerate(combined_data):\n",
    "        \n",
    "        original = example[\"original\"]\n",
    "        predicted = example[\"predicted\"]\n",
    "        text = example[\"text\"]\n",
    "        original_label_indexes = example[\"original_index\"]\n",
    "\n",
    "        # The text is tokenized\n",
    "        span_tokens = list(TreebankWordTokenizer().span_tokenize(text))\n",
    "        tokens = {k: text[i:j] for k, (i, j) in zip(span_tokens, span_tokens)}\n",
    "\n",
    "        # Predicted labels are split and assigned to tokens\n",
    "        predicted_labels = get_label_span(predicted, text, model, True)\n",
    "\n",
    "        if predicted_labels == None:\n",
    "            continue\n",
    "\n",
    "        # Original labels are split and assigned to tokens\n",
    "        original_labels = {}\n",
    "        \n",
    "        for key in original_label_indexes.keys():\n",
    "            for i in range(len(original_label_indexes[key])):\n",
    "                beginning = original_label_indexes[key][i][0]\n",
    "                parts = list(TreebankWordTokenizer().span_tokenize(original[key][i]))\n",
    "\n",
    "                # Entity consist of multiple tokens and is split\n",
    "                if len(parts) > 1:\n",
    "                    label_token_parts = list(TreebankWordTokenizer().tokenize(original[key][i]))\n",
    "                    \n",
    "                    for part in parts:\n",
    "                        new_index = (part[0] + int(beginning), part[1] + int(beginning))\n",
    "                        original_labels[new_index] =  key\n",
    "\n",
    "                else:\n",
    "                    original_labels[tuple(original_label_indexes[key][i])] = key\n",
    "\n",
    "        total_number_original_labels = list(original_labels.keys())\n",
    "\n",
    "        # Labels are formatted as a list \n",
    "        result_original = []\n",
    "        result_predicted = []\n",
    "        \n",
    "        for token in tokens:\n",
    "            \n",
    "            token_label_original = original_labels.get(token, 'O')\n",
    "            token_label_predicted = predicted_labels.get(token, 'O')\n",
    "            \n",
    "            # Some words are not tokenized equally so I try to match the most similar token \n",
    "            if token_label_original == \"O\" and token not in original_labels.keys() and token:\n",
    "                for k, v in original_labels.items():\n",
    "                    if k not in tokens.keys():\n",
    "                        if int(k[0]) >= token[0] and int(k[1]) <= token[1]:\n",
    "                            token_label_original = v\n",
    "                            total_number_original_labels.remove(k)\n",
    "                            break\n",
    "                            \n",
    "            if token_label_predicted == \"O\" and token not in predicted_labels.keys():\n",
    "                for k, v in predicted_labels.items():\n",
    "                    if k not in tokens.keys():\n",
    "                        if int(k[0]) >= token[0] and int(k[1]) <= token[1]:\n",
    "                            token_label_predicted = v\n",
    "                            break\n",
    "\n",
    "            if token_label_original != \"O\" and token in total_number_original_labels:\n",
    "                total_number_original_labels.remove(token)\n",
    "                \n",
    "            \n",
    "            result_original.append(token_label_original)\n",
    "            result_predicted.append(token_label_predicted)\n",
    "                \n",
    "        \n",
    "        original_by_token.append(result_original)\n",
    "        predicted_by_token.append(result_predicted)\n",
    "        examples_by_token.append(list(TreebankWordTokenizer().tokenize(text)))\n",
    "    \n",
    "    return original_by_token, predicted_by_token, examples_by_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7697c4b-6327-4ad5-8647-55886acfe1da",
   "metadata": {},
   "source": [
    "## N2c2 data evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f834ee0e-a3b5-4a44-9396-a193fc0bddd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = \"\"\n",
    "data_labels = [\"Drug\", \"Frequency\", \"Dosage\", \"ADE\", \"Reason\", \"Form\", \"Duration\", \"Route\", \"Strength\"]\n",
    "models = [\"GPT\", \"GPT-150\", \"GPT-300\", \"GPT-700\"]\n",
    "\n",
    "not_in_text = {}\n",
    "in_text = {}\n",
    "\n",
    "for model in models:\n",
    "    not_in_text[model] = {}\n",
    "    in_text[model] = {}\n",
    "    for label in data_labels:\n",
    "        not_in_text[model][label] = []\n",
    "        in_text[model][label] = 0\n",
    "        \n",
    "res_file = \"GPT_predictions.jsonl\"\n",
    "res_file150 = \"GPT-150_predictions.jsonl\"\n",
    "res_file300 = \"GPT-300_predictions.jsonl\"\n",
    "res_file700 = \"GPT-700_predictions.jsonl\"\n",
    "\n",
    "ot, pt, t = format_predictions(test_file, res_file)\n",
    "ot_150, pt_150, t_150 = format_predictions(test_file, res_file150)\n",
    "ot_300, pt_300, t_300 = format_predictions(test_file, res_file300)\n",
    "ot_700, pt_700, t_700 = format_predictions(test_file, res_file700)\n",
    "\n",
    "data_per_model = {}\n",
    "data_per_model[\"GPT\"] = [ot, pt, t]\n",
    "data_per_model[\"GPT-150\"] = [ot_150, pt_150, t_150]\n",
    "data_per_model[\"GPT-300\"] = [ot_300, pt_300, t_300]\n",
    "data_per_model[\"GPT-700\"] = [ot_700, pt_700, t_700]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea54214-dbea-4703-b748-956b55ea77d7",
   "metadata": {},
   "source": [
    "## Synthetic data evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e23b74-0b53-46a9-9d7b-b6d95475cb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = \"\"\n",
    "data_labels = [\"DISEASE\", \"DRUG\", \"SMOKING\", \"PROCEDURE\"]\n",
    "models = [\"GPT\", \"GPT-150\", \"GPT-300\", \"GPT-700\"]\n",
    "\n",
    "not_in_text = {}\n",
    "in_text = {}\n",
    "\n",
    "for model in models:\n",
    "    not_in_text[model] = {}\n",
    "    in_text[model] = {}\n",
    "    for label in data_labels:\n",
    "        not_in_text[model][label] = []\n",
    "        in_text[model][label] = 0\n",
    "\n",
    "res_file = \"GPT_synth_responses_format.jsonl\"\n",
    "res_file150 = \"GPT-150_synth_responses.jsonl\"\n",
    "res_file300 = \"GPT-300_synth_responses.jsonl\"\n",
    "res_file700 = \"GPT-700_synth_responses.jsonl\"\n",
    "\n",
    "ot, pt, t = format_predictions(test_file, res_file)\n",
    "ot_150, pt_150, t_150 = format_predictions(test_file, res_file150)\n",
    "ot_300, pt_300, t_300 = format_predictions(test_file, res_file300)\n",
    "ot_700, pt_700, t_700 = format_predictions(test_file, res_file700)\n",
    "\n",
    "data_per_model = {}\n",
    "data_per_model[\"GPT\"] = [ot, pt, t]\n",
    "data_per_model[\"GPT-150\"] = [ot_150, pt_150]\n",
    "data_per_model[\"GPT-300\"] = [ot_300, pt_300]\n",
    "data_per_model[\"GPT-700\"] = [ot_700, pt_700]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c1b6ef-1797-4623-bb21-6c68878ecafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_gpt = seqeval.compute(predictions=pt, references=ot)\n",
    "res_150 = seqeval.compute(predictions=pt_150, references=ot_150)\n",
    "res_300 = seqeval.compute(predictions=pt_300, references=ot_300)\n",
    "res_700 = seqeval.compute(predictions=pt_700, references=ot_700)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b642b56a-8170-44c9-a13d-2ba7108cb579",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccc3a1e-2bae-4cd1-9173-9f55686cd244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models_by_label(metrics, result_list, models, title):\n",
    "\n",
    "    m_dict = {}\n",
    "    m_dict[\"precision\"] = \"täpsus\"\n",
    "    m_dict[\"recall\"] = \"saagis\"\n",
    "    m_dict[\"f1\"] = \"f1-skoor\"\n",
    "\n",
    "    for metric in metrics:\n",
    "        # N2c2 data labels in Estonian\n",
    "        labels = [\"Üldine\", \"Kõrvalmõju\", \"Põhjus\", \"Vorm\", \"Doos\", \"Viis\", \"Sagedus\", \"Ravim\", \"Tugevus\", \"Kestus\"]\n",
    "        \n",
    "        # Synthetic data labels in Estonian\n",
    "        #labels = [\"Üldine\", \"Haigus\", \"Suitsetamine\", \"Protseduur\", \"Ravim\"] \n",
    "        \n",
    "        colors = ['blue', 'red', 'green', 'mediumvioletred']\n",
    "        bar_width = 0.175\n",
    "        \n",
    "        index = np.arange(len(labels))\n",
    "        plt.figure(figsize=(12, 6))\n",
    "\n",
    "        for i, res in enumerate(result_list):\n",
    "            values = [res[label][metric] for label in res.keys()]\n",
    "            plt.bar(index + i * bar_width, values, bar_width, label=models[i], color=colors[i])\n",
    "\n",
    "        plt.title(\"Mudelite \" + m_dict[metric] + \" \" + title + \" andmetel\")\n",
    "        plt.xticks(index + bar_width+ 0.074, labels)\n",
    "        plt.ylim(0, 1)\n",
    "        plt.legend()\n",
    "        plt.grid(axis = 'y')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2da10de-61ae-4d0d-9521-2a93a8237101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_labels(res):\n",
    "    '''\n",
    "    Function for fixing label names since seqeval package removes the first letters from labels.\n",
    "    '''\n",
    "    corrected_metrics = {}\n",
    "    corrected_metrics[\"Overall\"] = {}\n",
    "    \n",
    "    for label, metrics in res.items():\n",
    "        for data_label in data_labels:\n",
    "            if label.lower() in data_label.lower():\n",
    "                corrected_metrics[data_label] = metrics\n",
    "                break\n",
    "        \n",
    "        if \"overall\" in label.lower():\n",
    "            l = label.split(\"_\")[1]\n",
    "            if \"acc\" not in l:\n",
    "                corrected_metrics[\"Overall\"][l] = metrics\n",
    "    return corrected_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ba8865-a7ae-483e-aa4f-c8e070d9606d",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"GPT\", \"GPT-150\", \"GPT-300\", \"GPT-700\"]\n",
    "\n",
    "result_list_b = [res_gpt, res_150, res_300, res_700]\n",
    "result_list = [fix_labels(r) for r in result_list_b]\n",
    "\n",
    "'''\n",
    "n2c2 data evaluation\n",
    "'''\n",
    "#title = \"sünteetilistel\"\n",
    "#metrics = list(result_list[0]['DRUG'].keys())[:-1]\n",
    "\n",
    "'''\n",
    "synthetic data evaluation\n",
    "'''\n",
    "title = \"n2c2\"\n",
    "metrics = list(result_list[0]['Drug'].keys())[:-1]\n",
    "\n",
    "compare_models_by_label(metrics, result_list, models, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df373229-a86e-49a8-ac45-3aaa2d517579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting values\n",
    "recalls = [result['Overall']['recall'] for result in result_list]\n",
    "precisions = [result['Overall']['precision'] for result in result_list]\n",
    "f1 = [result['Overall']['f1'] for result in result_list]\n",
    "\n",
    "x_values = range(1, len(result_list) + 1)\n",
    "\n",
    "# Plotting\n",
    "plt.plot(x_values, recalls, marker='o')\n",
    "plt.plot(x_values, precisions, marker='o')\n",
    "\n",
    "# Adding labels and title\n",
    "x_values = range(1, len(result_list) + 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.xticks(x_values, models)\n",
    "\n",
    "plt.legend([\"Saagis\", \"Täpsus\", \"F1-skoor\"])\n",
    "plt.grid(axis = 'y')\n",
    "plt.xlabel('Mudel')\n",
    "plt.title('Mudelid n2c2 andmestikul')\n",
    "\n",
    "plt.savefig(\"mid_results/overall_plot_\"+ title +\"_est.jpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57527b9-3e5d-4822-8b41-ec8a12ebfe33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
