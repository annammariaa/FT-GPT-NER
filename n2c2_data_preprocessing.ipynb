{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ee4cf17-94f5-47a9-b195-573aa3503cba",
   "metadata": {},
   "source": [
    "# N2c2 data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73f6e70-b39b-4016-b725-a26c63d7dfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import tiktoken\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8deb0c41-5048-4123-87c2-621e8d6014a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pairs(directory):\n",
    "    '''\n",
    "    Text and annotation files are separate from each other. \n",
    "    This function creates filename pairs to ensure each text has a corresponding annotation file.\n",
    "    '''\n",
    "    pairs = []\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        \n",
    "        f = os.path.join(directory, filename)\n",
    "        \n",
    "        if os.path.isfile(f):\n",
    "            if f.endswith(\".txt\"):\n",
    "                \n",
    "                name = filename[:-4]\n",
    "                \n",
    "                if os.path.isfile(directory+name+\".ann\"):\n",
    "                    pairs.append((filename, name+\".ann\"))\n",
    "\n",
    "    \n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ca017e-cb0a-4f60-a505-7c06be5098e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shortening the anonymising tokens using regex patterns\n",
    "def clean_text(text):\n",
    "    \n",
    "    text = text.strip()\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = text.replace(\"\\t\", \"\")\n",
    "    text = text.replace(\". .\", \".\")\n",
    "\n",
    "    # The patterns are removed as they may reveal information about the data\n",
    "    pattern_date = r''\n",
    "    pattern_date2 = r''\n",
    "    pattern_date3 = r''\n",
    "    pattern_date4 = r''\n",
    "    pattern_year = r''\n",
    "    pattern_name = r''\n",
    "    pattern_hospital = r''\n",
    "    pattern_location = r''\n",
    "    pattern_contact = r''\n",
    "    pattern_space = r' +'\n",
    "    pattern_repeat = r'(.)\\1{3,}'\n",
    "\n",
    "    text = re.sub(pattern_date, \"*DATE*\", text)\n",
    "    text = re.sub(pattern_date2, \"*DATE*\", text)\n",
    "    text = re.sub(pattern_date3, \"*DATE*\", text)\n",
    "    text = re.sub(pattern_date4, \"*DATE*\", text)\n",
    "    text = re.sub(pattern_name, \"*NAME*\", text)\n",
    "    text = re.sub(pattern_hospital, \"*HOSPITAL*\", text)\n",
    "    text = re.sub(pattern_location, \"*LOC*\", text)\n",
    "    text = re.sub(pattern_contact, \"*CONTACT*\", text)\n",
    "    text = re.sub(pattern_year, \"*YEAR*\", text)\n",
    "    text = re.sub(pattern_space, \" \", text)\n",
    "    text = re.sub(pattern_repeat, \"\", text)\n",
    "\n",
    "    # Two or more consecutive tags are substituted by one\n",
    "    text = re.sub(r'(\\*DATE\\* )\\1{1,}', \"*DATE* \", text)\n",
    "    text = re.sub(r'(\\*NAME\\* )\\1{1,}', \"*NAME* \", text)\n",
    "    text = re.sub(r'(\\*LOC\\* )\\1{1,}', \"*LOC* \", text)\n",
    "    text = re.sub(r'(\\*HOSPITAL\\* )\\1{1,}', \"*HOSPITAL* \", text)\n",
    "    text = re.sub(r'(\\*CONTACT\\* )\\1{1,}', \"*CONTACT* \", text)\n",
    "    text = text.replace(\"[**\", \"*\")\n",
    "    text = text.replace(\"**]\", \"*\")\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6e9a86-398a-4418-8eb2-c6d4b06d0d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_annotations(f):\n",
    "    '''\n",
    "    Function for formatting the annotations into a JSON-format (label + beginning and end index).\n",
    "    '''\n",
    "    \n",
    "    data = {}\n",
    "    data_index = {}\n",
    "    lines = f.split(\"\\n\")\n",
    "    \n",
    "    for line in lines:\n",
    "        # The line contains annotation\n",
    "        if line.startswith(\"T\"):\n",
    "            \n",
    "            parts = line.split(\"\\t\")\n",
    "            info = parts[1].split(\" \") #indexes\n",
    "            \n",
    "            tag = info[0] #entity label\n",
    "            value = parts[2] # entity\n",
    "\n",
    "            if len(info) > 3:\n",
    "                split = info[2].split(\";\")\n",
    "                if split[0] == split[1]:\n",
    "                    index = int(split[0]) - int(info[1])\n",
    "                    value = value[:index] + value[index+1:]\n",
    "                    \n",
    "            value = clean_text(value)\n",
    "\n",
    "            # Adding to dictionaries\n",
    "            if tag in data.keys():\n",
    "                data[tag].append(value)\n",
    "                data_label_index_tup = (info[1], info[-1])\n",
    "                data_index[tag].append(data_label_index_tup)\n",
    "            else:\n",
    "                data[tag] = [value]\n",
    "                data_label_index_tup = (info[1], info[-1])\n",
    "                data_index[tag] = [data_label_index_tup]\n",
    "    \n",
    "    # Empty lists for entites not found in text         \n",
    "    for t in tags:\n",
    "        if t not in data.keys():\n",
    "            data[t] = []\n",
    "            data_index[t] = []\n",
    "    \n",
    "    return data, data_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db69cc5-958c-4e6d-b001-9cb5d7f294bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_messages(text, ann, ann_i):\n",
    "    '''\n",
    "    Function for formatting message examples.\n",
    "    Annotations by index and full text are included for testing and have to be removed when creating the fine-tuning file.\n",
    "    '''\n",
    "    messages = []\n",
    "\n",
    "    # System prompt\n",
    "    sys = {}\n",
    "    sys[\"role\"] = \"system\"\n",
    "    sys[\"content\"] = system_content\n",
    "\n",
    "    # Model prompt and text\n",
    "    user = {}\n",
    "    user[\"role\"] = \"user\"\n",
    "    user[\"content\"] = instruction + clean_text(text)\n",
    "\n",
    "    # Annotations\n",
    "    ass = {}\n",
    "    ass[\"role\"] = \"assistant\"\n",
    "    ass[\"content\"] = ann\n",
    "\n",
    "    # Annotations by index\n",
    "    ann_index = {}\n",
    "    ann_index[\"role\"] = \"annotations\"\n",
    "    ann_index[\"content\"] = ann_i\n",
    "\n",
    "    # Text without changes so the annotation indexes match the text\n",
    "    full_text = {}\n",
    "    full_text[\"role\"] = \"full_user\"\n",
    "    full_text[\"content\"] = text\n",
    "\n",
    "    messages.append(sys)\n",
    "    messages.append(user)\n",
    "    messages.append(ass)\n",
    "    messages.append(ann_index)\n",
    "    messages.append(full_text)\n",
    "\n",
    "    example = {}\n",
    "    example[\"messages\"] = messages\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c0eb32-85ff-4ae0-bda0-fbe8fd027d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_example(pair):\n",
    "    '''\n",
    "    Function for creating an example of a data file pair.\n",
    "    '''\n",
    "    ann_file = pair[1]\n",
    "    text_file = pair[0]\n",
    "    \n",
    "    with open(directory+text_file) as file0:\n",
    "        text = file0.read()\n",
    "    \n",
    "    with open(directory+ann_file) as file1:\n",
    "        f = file1.read()\n",
    "        ann, ann_i = get_annotations(f)\n",
    "    \n",
    "    example = format_messages(text, ann, ann_i)\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b4caa5-fe16-43d5-bf05-9be146e7b8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_within_limit(example):\n",
    "    '''\n",
    "    Function for ensuring that the example is within models context limit.\n",
    "    '''\n",
    "    messages = example[\"messages\"]\n",
    "\n",
    "    total_tokens = 0\n",
    "    for message in messages:\n",
    "        if message[\"role\"] != \"annotations\" and message[\"role\"] != \"full_user\": # Excluded as they are not used in fine-tuning\n",
    "            total_tokens += len(encoding.encode(str(message[\"content\"])))\n",
    "\n",
    "    if total_tokens <= token_limit:\n",
    "        return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffc313f-2caa-4808-ad3a-52f789b8b1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kui mõni märgendus peaks jääma täpselt kahe teksti vahele, võtan selle alguse eelmise teksti lõpppunktiks ning alustan uut näidet selle algusest\n",
    "def split_example_into_parts(pair, num_parts):\n",
    "\n",
    "    '''\n",
    "    Function that splits text and annotations into multiple parts (num_parts).\n",
    "\n",
    "    The annotations are in a random order, therefore, the whole file has to be parsed for each part.\n",
    "    '''\n",
    "    \n",
    "    examples_after_split = []\n",
    "\n",
    "    with open(directory+pair[1]) as file1:\n",
    "        ann_file = file1.read()\n",
    "\n",
    "    with open(directory+pair[0]) as file0:\n",
    "        text_file = file0.read()\n",
    "\n",
    "    # The maximum length of text parts\n",
    "    len_texts = math.ceil(len(text_file) / num_parts)\n",
    "    \n",
    "    beginning = 0\n",
    "    stuck_in_the_middle_b = 0\n",
    "\n",
    "    lines = ann_file.split(\"\\n\")\n",
    "    \n",
    "    for i in range(num_parts):\n",
    "        \n",
    "        end = beginning + len_texts\n",
    "        data = {}\n",
    "        data_index = {}\n",
    "        middle_of_tag = False\n",
    "\n",
    "        for line in lines:\n",
    "            if line.startswith(\"T\"):\n",
    "                \n",
    "                parts = line.split(\"\\t\")\n",
    "                info = parts[1].split(\" \")\n",
    "\n",
    "                tag = info[0] # entity label\n",
    "                b = int(info[1]) # entity beginning index\n",
    "                e = int(info[-1])  # entity end index\n",
    "\n",
    "                value = parts[2]\n",
    "    \n",
    "                if len(info) > 3:\n",
    "                    split = info[2].split(\";\")\n",
    "                    if split[0] == split[1]:\n",
    "                        index = int(split[0]) - int(info[1])\n",
    "                        value = value[:index] + value[index+1:]\n",
    "\n",
    "                value = clean_text(value)\n",
    "                \n",
    "                # Entity is in the current text part\n",
    "                if b >= beginning and e <= end:\n",
    "                    if tag in data.keys():\n",
    "                        data[tag].append(value)\n",
    "                        data_label_index_tup = (b-beginning, e-beginning)\n",
    "                        data_index[tag].append(data_label_index_tup)\n",
    "                    else:\n",
    "                        data[tag] = [value]\n",
    "                        data_label_index_tup = (b-beginning, e-beginning)\n",
    "                        data_index[tag] = [data_label_index_tup]\n",
    "                        \n",
    "                # An entity would be split between two parts, so the enity is moved to the second part\n",
    "                elif end > b and b >= beginning and e > end:\n",
    "                    stuck_in_the_middle_b = b\n",
    "                    middle_of_tag = True\n",
    "                    \n",
    "        for t in tags:\n",
    "            if t not in data.keys():\n",
    "                data[t] = []\n",
    "                data_index[t] = []\n",
    "        \n",
    "        if middle_of_tag:\n",
    "            text = text_file[beginning:stuck_in_the_middle_b]\n",
    "            beginning = stuck_in_the_middle_b\n",
    "        else:\n",
    "            text = text_file[beginning:end]\n",
    "            beginning = end\n",
    "\n",
    "        example = format_messages(text, data, data_index)\n",
    "        \n",
    "        if example_within_limit(example):\n",
    "            if no_mistakes_in_example(example):\n",
    "                examples_after_split.append(example)\n",
    "    \n",
    "    return examples_after_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232fedf2-e156-4e4f-8635-7541f57aade2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_split_number(example):\n",
    "    '''\n",
    "    Function for calculating the number of parts a text should be split into.\n",
    "    I ensure that each text part, along with all its annotations, stays within the limit, as all entities might be contained within a single part.\n",
    "    '''\n",
    "    \n",
    "    text_tokens = len(encoding.encode(str(example[\"messages\"][1][\"content\"]))) - len(encoding.encode(instruction))\n",
    "    tag_tokens = len(encoding.encode(str(example[\"messages\"][2][\"content\"]))) - len(encoding.encode(assistant_base))\n",
    "\n",
    "    num_parts = math.ceil(text_tokens / available_tokens) if math.ceil(text_tokens / available_tokens) > 2 else 2\n",
    "    \n",
    "    in_limit = False\n",
    "    \n",
    "    while not in_limit:\n",
    "        \n",
    "        split_text_tokens = math.floor(text_tokens/num_parts)\n",
    "        new_example_tokens = split_text_tokens + tag_tokens\n",
    "        \n",
    "        if new_example_tokens <= available_tokens:\n",
    "            in_limit = True\n",
    "        else: \n",
    "            num_parts += 1\n",
    "            \n",
    "    return num_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c332da-fdf6-4a4e-81cd-08ec12b89437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_mistakes_in_example(example):\n",
    "    '''\n",
    "    Function for ensuring that all entites are present in corresponding text.\n",
    "    '''\n",
    "    all_tags_in_text = True\n",
    "\n",
    "    messages = example[\"messages\"]\n",
    "    text = messages[1][\"content\"]\n",
    "    ann = messages[2][\"content\"]\n",
    "\n",
    "    for key, value in ann.items():\n",
    "        for item in value:\n",
    "            item = item.strip()\n",
    "            if item not in text:\n",
    "                all_tags_in_text = False\n",
    "        \n",
    "    return all_tags_in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4880d744-7a20-4db2-94b6-9d70f50778a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_examples_in_limit(pairs):\n",
    "    '''\n",
    "    Function that creates a formatted example and splits the examples that are over the context limit.\n",
    "    '''\n",
    "    \n",
    "    examples = []\n",
    "    \n",
    "    for pair in pairs:\n",
    "        example = create_example(pair)\n",
    "        \n",
    "        if example_within_limit(example):\n",
    "            if no_mistakes_in_example(example):\n",
    "                \texamples.append(example)\n",
    "        else:\n",
    "            num_parts = calculate_split_number(example)  \n",
    "            examples += split_example_into_parts(pair, num_parts)\n",
    "    \n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7c04fb-81f3-47c5-9125-9f28997e28ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_examples_to_file(filename, examples):\n",
    "    '''\n",
    "    Function for writing examples into a JSONL-file.\n",
    "    '''\n",
    "    with open(filename, \"w\") as file:\n",
    "        for example in examples:\n",
    "            json.dump(example, file)\n",
    "            file.write('\\n')\n",
    "    \n",
    "    files.append(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502279f7-416b-495c-8a76-84694c92dfbb",
   "metadata": {},
   "source": [
    "# Data formatting and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6e14d1-499a-4183-933d-89dd5751e835",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"\"\n",
    "folder = \"\"\n",
    "\n",
    "# Path to original data\n",
    "directory = path + folder\n",
    "\n",
    "# Test-train/val split\n",
    "test_size = 0.15\n",
    "\n",
    "token_limit = 4063\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "tags = [\"Drug\", \"Strength\", \"Dosage\", \"Duration\", \"Frequency\", \"Form\", \"Route\", \"Reason\", \"ADE\"]\n",
    "\n",
    "system_content = \"This model extracts entities from text, returning JSON-formatted output for tags \" + \", \".join(tags) + \".\"\n",
    "instruction = \"Extract entities \" + \", \".join(tags) + \" from the following text and return the output in JSON format. \"\n",
    "assistant_base = \"{\" + \":[], \".join(tags) + \":[]}\"\n",
    "\n",
    "available_tokens = token_limit - (len(encoding.encode(system_content)) + len(encoding.encode(instruction)) + len(encoding.encode(assistant_base)))\n",
    "\n",
    "files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038d722c-bc2f-4cf1-8a2f-844f5f544e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text and annotation file pairs\n",
    "pairs = create_pairs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49207be-6462-4010-831b-52de8e98cbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test and train split\n",
    "test_train_split = round(len(pairs) * test_size)\n",
    "\n",
    "train_pairs = pairs[test_train_split:]\n",
    "test_pairs = pairs[:test_train_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e8f11c-3faf-4ef8-842c-0a7592ea0b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the examples\n",
    "train_data = create_examples_in_limit(train_pairs)\n",
    "test_data = create_examples_in_limit(test_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00104b4-361a-43b2-84f3-228233b09ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformatting the test examples to match synthetic data examples\n",
    "\n",
    "full_test_data = []\n",
    "\n",
    "for example in test_data:\n",
    "    fixed_example = {}\n",
    "    parts = example[\"messages\"]\n",
    "\n",
    "    new_messages = []\n",
    "    new_messages.append(parts[0])\n",
    "    user_text = parts[-1]\n",
    "    user_text[\"role\"] = \"user\"\n",
    "    new_messages.append(user_text)\n",
    "    new_messages.append(parts[2])\n",
    "    new_messages.append(parts[3])\n",
    "\n",
    "    fixed_example[\"messages\"] = new_messages\n",
    "\n",
    "    full_test_data.append(fixed_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13db9482-fa9e-40f8-a101-7e0874f0631b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the examples into a files\n",
    "\n",
    "write_examples_to_file(\"\", train_data)\n",
    "write_examples_to_file(\"\", full_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c0fca7-1389-4dbd-b6c6-5f20f3e10f2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
